{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "397bb0c9",
   "metadata": {},
   "source": [
    "## Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383e015d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31df6033",
   "metadata": {},
   "source": [
    "## Load words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f43a425",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_words = pd.DataFrame(\n",
    "    open(\"../data/negative-words.txt\", encoding=\"latin-1\").read().splitlines(),\n",
    "    columns=[\"word\"]\n",
    ")\n",
    "\n",
    "pos_words = pd.DataFrame(\n",
    "    open(\"../data/positive-words.txt\", encoding=\"latin-1\").read().splitlines(),\n",
    "    columns=[\"word\"]\n",
    ")\n",
    "\n",
    "neg_words[\"sentiment\"] = 0\n",
    "pos_words[\"sentiment\"] = 1\n",
    "\n",
    "neg_set = set(neg_words['word'])\n",
    "pos_set = set(pos_words['word'])\n",
    "\n",
    "\n",
    "#combine datasets\n",
    "word_df = pd.concat([pos_words, neg_words], ignore_index=True)\n",
    "\n",
    "#shuffle rows\n",
    "word_df = word_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "word_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f81a8d",
   "metadata": {},
   "source": [
    "## Preparing our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23f87a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "neg_reviews = pd.DataFrame(\n",
    "    open(\"../data/negative-reviews.txt\", encoding=\"latin-1\").read().splitlines(),\n",
    "    columns=[\"review\"]\n",
    ")\n",
    "\n",
    "pos_reviews = pd.DataFrame(\n",
    "    open(\"../data/positive-reviews.txt\", encoding=\"latin-1\").read().splitlines(),\n",
    "    columns=[\"review\"]\n",
    ")\n",
    "\n",
    "neg_reviews[\"sentiment\"] = 0\n",
    "pos_reviews[\"sentiment\"] = 1\n",
    "\n",
    "# Split dataset into training and testing sets (80-20)\n",
    "neg_train, neg_test, pos_train, pos_test = train_test_split(neg_reviews, pos_reviews, test_size=0.2, random_state=42)\n",
    "\n",
    "# Combine and shuffle train set\n",
    "train =  pd.concat([neg_train, pos_train], ignore_index=True)\n",
    "train = train.sample(frac=1).reset_index(drop=True) \n",
    "\n",
    "# Combine and shuffle test set\n",
    "test = pd.concat([neg_test, pos_test],)\n",
    "test = test.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print(\"Shuffled train set: \\n\", train.head())\n",
    "print('\\n')\n",
    "print(\"Shuffled test set: \\n\", test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e192385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    contractions = {\n",
    "        \"doesn't\": \"does not\", \"don't\": \"do not\", \"isn't\": \"is not\",\n",
    "        \"wasn't\": \"was not\", \"didn't\": \"did not\", \"won't\": \"will not\",\n",
    "        \"can't\": \"can not\", \"couldn't\": \"could not\", \"shouldn't\": \"should not\",\n",
    "        \"wouldn't\": \"would not\", \"haven't\": \"have not\", \"hasn't\": \"has not\",\n",
    "        \"hadn't\": \"had not\", \"aren't\": \"are not\", \"weren't\": \"were not\"\n",
    "    }\n",
    "    text_lower = text.lower()\n",
    "    for contraction, expansion in contractions.items():\n",
    "        text_lower = text_lower.replace(contraction, expansion)\n",
    "    return text_lower\n",
    "\n",
    "# Apply before feature extraction\n",
    "train[\"review\"] = train[\"review\"].apply(preprocess_text)\n",
    "test[\"review\"] = test[\"review\"].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c13fedb",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659b2f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "pronouns = {\"i\", \"me\", \"my\", \"you\", \"your\"}\n",
    "\n",
    "def extract_features(review):\n",
    "    words = review.lower().split()\n",
    "    word_count = len(words)\n",
    "    has_exclaim = int(\"!\" in review)\n",
    "    pronoun_count = sum(1 for w in words if w in pronouns)\n",
    "    pos_count = sum(1 for w in words if w in pos_set)\n",
    "    neg_count = sum(1 for w in words if w in neg_set)\n",
    "    has_no = int(\"no\" in words)\n",
    "    log_length = math.log(word_count + 1)\n",
    "    question_count = review.count(\"?\")\n",
    "    \n",
    "    sentiment_ratio = (pos_count - neg_count) / (word_count + 1)\n",
    "    intensifiers = {\"very\", \"extremely\", \"really\", \"absolutely\", \"totally\", \"completely\"}\n",
    "    intensifier_count = sum(1 for w in words if w in intensifiers)\n",
    "    \n",
    "    negations = {\"not\", \"never\", \"neither\", \"nobody\", \"nothing\", \"nowhere\", \"no\"}\n",
    "    negation_count = sum(1 for w in words if w in negations)\n",
    "    \n",
    "    caps_count = sum(1 for w in review.split() if w.isupper() and len(w) > 1)\n",
    "    avg_word_len = sum(len(w) for w in words) / (word_count + 1)\n",
    "\n",
    "    \n",
    "    return [\n",
    "        has_exclaim, \n",
    "        pronoun_count, \n",
    "        pos_count, \n",
    "        neg_count, \n",
    "        has_no,\n",
    "        log_length, \n",
    "        question_count, \n",
    "        sentiment_ratio, \n",
    "        intensifier_count,\n",
    "        negation_count, \n",
    "        caps_count, \n",
    "        avg_word_len\n",
    "    ]\n",
    "    \n",
    "# Apply to all reviews\n",
    "train_extra = np.array([extract_features(r) for r in train[\"review\"]])\n",
    "test_extra  = np.array([extract_features(r) for r in test[\"review\"]])\n",
    "\n",
    "# Standardize extra features\n",
    "scaler = StandardScaler()\n",
    "train_extra = scaler.fit_transform(train_extra)\n",
    "test_extra  = scaler.transform(test_extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1649bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=10000,      \n",
    "    ngram_range=(1,3),       \n",
    "    stop_words='english',\n",
    "    min_df=2,                \n",
    "    max_df=0.8,              \n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "# Fit TF-IDF on training reviews and transform\n",
    "X_train_tfidf = vectorizer.fit_transform(train[\"review\"])\n",
    "X_test_tfidf  = vectorizer.transform(test[\"review\"])\n",
    "\n",
    "# Labels\n",
    "y_train = train[\"sentiment\"].tolist()\n",
    "y_test  = test[\"sentiment\"].tolist()\n",
    "\n",
    "# hstack combines sparse TF-IDF with dense extra features\n",
    "X_train = hstack([X_train_tfidf, train_extra])\n",
    "X_test  = hstack([X_test_tfidf, test_extra])\n",
    "\n",
    "y_train = train[\"sentiment\"].tolist()\n",
    "y_test  = test[\"sentiment\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73cce85",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f1c39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
